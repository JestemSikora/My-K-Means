{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11661c2c",
   "metadata": {},
   "source": [
    "# My K-Means Implementation\n",
    "\n",
    "    1. Notes for K-Means\n",
    "    2. My implementation from scratch and using library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570df3f",
   "metadata": {},
   "source": [
    "## 1. Notes for K-Means\n",
    "    A. Main idea\n",
    "    B. How it works\n",
    "    C. When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb140c",
   "metadata": {},
   "source": [
    "### A. Main idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0230200",
   "metadata": {},
   "source": [
    "K-Means is an unsupervised Machine Learning algorithm. It finds patterns in the input features and groups the data into clusters. Based on the mean position of data points (centroids), it assigns each point to the nearest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0ddfb",
   "metadata": {},
   "source": [
    "### B. How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526111d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "1. Choose the number of clusters\n",
    "We decide on a parameter **k** (number of clusters). \n",
    "\n",
    "---\n",
    "\n",
    "2. Initialize centroids\n",
    "Randomly choose **k** points from the dataset (or random positions in feature space, even if these points do not exist in the dataset) as the initial **centroids**.\n",
    "\n",
    "â†’ **Centroids** are points representing the center of the cluster.\n",
    "\n",
    "---\n",
    "\n",
    "3. Assign points to clusters\n",
    "For each data point \\(x_i\\), assign it to the closest centroid using **Euclidean distance**:\n",
    "\n",
    "$$\n",
    "d(x_i, \\mu_j) = \\sqrt{ \\sum_{f=1}^{n} \\left( x_{i,f} - \\mu_{j,f} \\right)^2 }\n",
    "$$\n",
    "\n",
    "where:\n",
    "- x_{i,f} - our (i)th data point of (f)th feature \n",
    "- \\(u{j,f}\\) â€“ our (j)th randomly selected centroid of (f)th feature \n",
    "\n",
    "-> our model iterates through every feature, because we can not perform calculation between diffrent features and diffrent points.\n",
    "\n",
    "The cluster assignment is:\n",
    "\n",
    "$$\n",
    "c_i = \\arg \\min_{j \\in \\{1, \\dots, k\\}} d(x_i, \\mu_j)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "4. Update centroids\n",
    "After all points are assigned, compute new centroids by taking the mean of all points in each cluster:\n",
    "\n",
    "$$\n",
    "\\mu_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- (C_j) is the set of points assigned to cluster j\n",
    "  \n",
    "---\n",
    "\n",
    "5. Repeat steps 3â€“4\n",
    "Reassign points and update centroids until:\n",
    "- The centroids do not change significantly  \n",
    "- Or a maximum number of iterations is reached  \n",
    "\n",
    "---\n",
    "\n",
    "6. Objective Function (to minimize)\n",
    "K-Means minimizes the **within-cluster sum of squares (WCSS):**\n",
    "\n",
    "$$\n",
    "J = \\sum_{j=1}^{k} \\sum_{x_i \\in C_j} \\| x_i - \\mu_j \\|^2\n",
    "$$\n",
    "\n",
    "This is the measure of how \"tight\" the clusters are around their centroids.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04eb2c4",
   "metadata": {},
   "source": [
    "### C. When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50eb418",
   "metadata": {},
   "source": [
    "\n",
    "ðŸ¥³ *K-Means is a good choice when:*\n",
    "\n",
    "- You want to **discover hidden structure** in unlabeled data.  \n",
    "- You expect the data to form **roughly spherical clusters** of similar size.  \n",
    "- You need a **fast and scalable algorithm** (K-Means is computationally efficient).  \n",
    "- The number of clusters \\(k\\) is known (or you can estimate it with methods like the **Elbow Method** or **Silhouette Score**).  \n",
    "- You work with data in **low to medium dimensions** (works best when not too many features).  \n",
    "\n",
    "\n",
    "\n",
    "*When to Be Careful?* ðŸ‘¾\n",
    "\n",
    "- If clusters are **not spherical** (e.g., elongated or complex shapes), K-Means may perform poorly.  \n",
    "- If clusters have **very different sizes or densities**, results may be misleading.  \n",
    "- Sensitive to **outliers**, since they can pull the centroid far away.  \n",
    "- Sensitive to **initialization** â€“ different random seeds may give different results.  \n",
    "  - Solution: use **K-Means++ initialization**.  \n",
    "- In **high-dimensional space** (curse of dimensionality), distances lose meaning and clustering quality degrades.\n",
    "\n",
    "\n",
    "-> spherical clusters are desired because of it's distribution near the center!\n",
    "  if clusters are not this way, use: **DBSCAN** or **Spectral Clustering**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6a86c",
   "metadata": {},
   "source": [
    "## 2. My implementation from scratch and using library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f854d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from kmeans_model import My_Kmeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score, normalized_mutual_info_score, fowlkes_mallows_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99473c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 0 1 1 2 0 0 1 1 0 2 2 2 2 2 1 0 0 2 0 0 1 1 1 1 2 1 2 0 2 1 0 0 2 1\n",
      " 2 2 0 1 1 2 0 2 1 1 0 2 2 0 0 1 1 2 0 0 1 0 1 2 0 2 0 0 1 0 0 1 2 1 1 1 0\n",
      " 0 1 2 0 0 1 1 1 2 1 1 1 2 0 0 1 2 2 2 2 0 1 0 1 1 0 1 2 1 2 2 0 1 0 2 2 1\n",
      " 1 2 2 1 0 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Loading data from sklearn load_iris \n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=123)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f7d130d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering for 0th iteration: ['0th: 45', '1th: 13', '2th: 62']\n",
      "New position after 0th iteration: [[5.31 2.75 3.29 0.97]\n",
      " [5.68 3.75 2.83 0.99]\n",
      " [6.31 3.1  4.41 1.46]]\n",
      "---------------------------------------------\n",
      "Clustering for 5th iteration: ['0th: 34', '1th: 37', '2th: 49']\n",
      "New position after 5th iteration: [[5.71 2.67 4.1  1.27]\n",
      " [5.02 3.43 1.47 0.26]\n",
      " [6.62 3.   5.41 1.92]]\n",
      "---------------------------------------------\n",
      "Clustering for 10th iteration: ['0th: 43', '1th: 37', '2th: 40']\n",
      "New position after 10th iteration: [[5.83 2.73 4.23 1.34]\n",
      " [5.02 3.43 1.47 0.26]\n",
      " [6.7  3.01 5.56 1.99]]\n",
      "---------------------------------------------\n",
      "Clustering for 15th iteration: ['0th: 49', '1th: 37', '2th: 34']\n",
      "New position after 15th iteration: [[5.87 2.74 4.32 1.39]\n",
      " [5.02 3.43 1.47 0.26]\n",
      " [6.79 3.05 5.67 2.04]]\n",
      "---------------------------------------------\n",
      "Clustering for 20th iteration: ['0th: 50', '1th: 37', '2th: 33']\n",
      "New position after 20th iteration: [[5.88 2.74 4.33 1.4 ]\n",
      " [5.02 3.43 1.47 0.26]\n",
      " [6.81 3.06 5.69 2.04]]\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# k -> Number of Clusters\n",
    "k = 3\n",
    "\n",
    "# Importing and using my implementation of K-Means Algorithm\n",
    "model_1 = My_Kmeans()\n",
    "model_1.fit(X_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069a1d1",
   "metadata": {},
   "source": [
    "Because of unsupervised algorithm, to chceck how our model acts on a new data, we need to use diffrent metrics methods than usually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a445113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted mutual info score: 74.0 %\n",
      "Normalized mutual info score: 75.0 %\n",
      "Fowlkes mallows score: 82.0 %\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_1.k\n",
    "\n",
    "# Changing from dict to segregated list by points, as is in X_train\n",
    "ordered_labels = []\n",
    "for x in X_train:\n",
    "    found = False\n",
    "    for cluster_id, points in y_train_pred.items():\n",
    "        if any(np.array_equal(x, p) for p in points):\n",
    "            cluster_id = int(cluster_id[0:1])\n",
    "            ordered_labels.append(cluster_id)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        ordered_labels.append(None)\n",
    "\n",
    "# Checking how well our model works\n",
    "ari = adjusted_mutual_info_score(y_train, ordered_labels)\n",
    "print(f'Adjusted mutual info score: {round(ari,2)* 100} %')\n",
    "\n",
    "nmi = normalized_mutual_info_score(y_train, ordered_labels)\n",
    "print(f'Normalized mutual info score: {round(nmi,2)* 100} %')\n",
    "\n",
    "fmi = fowlkes_mallows_score(y_train, ordered_labels)\n",
    "print(f'Fowlkes mallows score: {round(fmi,2)* 100} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd36eea",
   "metadata": {},
   "source": [
    "\n",
    "    -> ARI - checks how well predicted clasters matches with real classes\n",
    "\n",
    "    -> NMI - checks how simillar two clasters are\n",
    "\n",
    "    -> FMI - checks compliance of range for each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "149ed9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted mutual info score: 73.0 %\n",
      "Normalized mutual info score: 74.0 %\n",
      "Fowlkes mallows score: 81.0 %\n"
     ]
    }
   ],
   "source": [
    "model_2 = KMeans(n_clusters=3, random_state=123)\n",
    "y_sklearn_pred = model_2.fit_predict(X_train)\n",
    "\n",
    "y_sklearn_pred\n",
    "ari = adjusted_mutual_info_score(y_train, y_sklearn_pred)\n",
    "print(f'Adjusted mutual info score: {round(ari,2)* 100} %')\n",
    "\n",
    "nmi = normalized_mutual_info_score(y_train, y_sklearn_pred)\n",
    "print(f'Normalized mutual info score: {round(nmi,2)* 100} %')\n",
    "\n",
    "fmi = fowlkes_mallows_score(y_train, y_sklearn_pred)\n",
    "print(f'Fowlkes mallows score: {round(fmi,2)* 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67b800",
   "metadata": {},
   "source": [
    "# Clustering Evaluation Metrics\n",
    "\n",
    "When evaluating clustering algorithms such as **K-Means**, we often need to compare the predicted clusters with the **true labels** (ground truth).  \n",
    "Since cluster labels can be permuted (e.g., cluster \"0\" may correspond to class \"virginica\"), we use special metrics that are invariant to label permutation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Adjusted Rand Index (ARI)\n",
    "\n",
    "The **Rand Index (RI)** measures similarity between two clusterings by counting pairs of points that are:\n",
    "- in the same cluster in both labelings, or\n",
    "- in different clusters in both labelings.\n",
    "\n",
    "Formally, for all pairs of samples:\n",
    "\n",
    "- \\(a\\): pairs in the same cluster in both partitions  \n",
    "- \\(b\\): pairs in different clusters in both partitions  \n",
    "- \\(c\\): pairs in the same cluster in first but different in second  \n",
    "- \\(d\\): pairs in different clusters in first but same in second  \n",
    "\n",
    "The **Rand Index** is:\n",
    "\n",
    "$$\n",
    "RI = \\frac{a + b}{a + b + c + d}\n",
    "$$\n",
    "\n",
    "However, RI does not account for random labeling.  \n",
    "Therefore, the **Adjusted Rand Index (ARI)** corrects for chance:\n",
    "\n",
    "$$\n",
    "ARI = \\frac{RI - \\mathbb{E}[RI]}{\\max(RI) - \\mathbb{E}[RI]}\n",
    "$$\n",
    "\n",
    "- Range: \\([-1, 1]\\)  \n",
    "- \\(1\\) â†’ perfect agreement  \n",
    "- Around \\(0\\) â†’ random labeling  \n",
    "- Negative values â†’ worse than random  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Normalized Mutual Information (NMI)\n",
    "\n",
    "Mutual Information (MI) comes from information theory â€“ it measures how much knowing one clustering reduces uncertainty about the other.  \n",
    "\n",
    "Given clusterings \\(U\\) and \\(V\\):\n",
    "\n",
    "$$\n",
    "MI(U, V) = \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \n",
    "\\frac{|U_i \\cap V_j|}{N} \\, \\log \\left( \\frac{N \\cdot |U_i \\cap V_j|}{|U_i| \\cdot |V_j|} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(N\\) â€“ total number of samples  \n",
    "- \\(|U_i|\\) â€“ number of points in cluster \\(i\\) in partition \\(U\\)  \n",
    "- \\(|V_j|\\) â€“ number of points in cluster \\(j\\) in partition \\(V\\)  \n",
    "\n",
    "To normalize between 0 and 1, we use:\n",
    "\n",
    "$$\n",
    "NMI(U, V) = \\frac{MI(U, V)}{\\sqrt{H(U) \\cdot H(V)}}\n",
    "$$\n",
    "\n",
    "where \\(H(U)\\), \\(H(V)\\) are the entropies of the partitions.  \n",
    "\n",
    "- Range: \\([0, 1]\\)  \n",
    "- \\(1\\) â†’ perfect agreement  \n",
    "- \\(0\\) â†’ completely independent clusterings  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Fowlkes-Mallows Index (FMI)\n",
    "\n",
    "The **FMI** is based on precision and recall for clustering pairs.  \n",
    "\n",
    "- **TP (True Positive):** pairs in the same cluster in both partitions  \n",
    "- **FP (False Positive):** pairs in same cluster in predicted, different in true  \n",
    "- **FN (False Negative):** pairs in different cluster in predicted, same in true  \n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "FMI = \\sqrt{ \\frac{TP}{TP + FP} \\cdot \\frac{TP}{TP + FN} }\n",
    "$$\n",
    "\n",
    "- Range: \\([0, 1]\\)  \n",
    "- \\(1\\) â†’ perfect clustering  \n",
    "- Values closer to 0 â†’ poor clustering  \n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Metric | Range | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| **ARI** | \\([-1, 1]\\) | 1 = perfect, 0 = random, <0 = worse than random |\n",
    "| **NMI** | \\([0, 1]\\)  | 1 = perfect, 0 = independent |\n",
    "| **FMI** | \\([0, 1]\\)  | 1 = perfect, 0 = poor clustering |\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Notes\n",
    "- All three metrics are **invariant to label permutation** (cluster 0 vs 1 naming does not matter).  \n",
    "- **ARI** adjusts for chance and can go below 0.  \n",
    "- **NMI** is based on information theory and is useful for comparing different numbers of clusters.  \n",
    "- **FMI** focuses on pairwise precision/recall balance.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-DA-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
